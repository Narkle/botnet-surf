import sys, os
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, LSTM, Flatten
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import random 
from sklearn.metrics import roc_curve, auc
from graph_tool.all import *

import prep_time_series_input
import scenario_info
import create_graph
from metrics import *
from self_organizing_map import SOM
# import neural_network_som

"""
Vertex features - all 12 of these are calculated using graph-tool's functions:
["Out-degree", "In-degree", "# of in-neighbors", "# of out-neighbors", 
 "Page Rank", "Betweenness", "Closeness", "Eigenvector", "Katz",
 "Authority centrality", "Hub centrality", "Clustering coefficient"]
The above features will be normalized and placed in a vector for each vertex
in each time interval
"""

VECTOR_SIZE = 12 # number of vertex characteristics in the vector

# Disable print statements
def blockPrint():
    sys.stdout = open(os.devnull, 'w')

# Enable print stements
def enablePrint():
    sys.stdout = sys.__stdout__

'''
Trains the model
Parameters:
x_train - NumPy array for x training set
y_train - NumPy array for y training set
pcap_duration - pcap duration (seconds) - available on CTU website
step_length - step duration (seconds)
save_model - True if model is saved in an h5 file
savefile - name of file that the model is saved to
'''
def create_model(x_train, y_train, num_samples, windows_per_sample, \
	save_model=True, savefile="model.h5"):
	print "Starting the creation of the model"
	model = Sequential()
	# Input arrays of shape (num_vertices, 12) and
	# output arrays of shape (num_vertices, 1)
	# len(x_train) = number of samples/vertices
	# len(x_train[0]) = number of time_steps/graphs,
	# len(x_train[0][0]) = number of features
	'''
	# Adding batch size screws up the program since it has to match batch
	# size later...it's necessary for stateful LSTM but not for stateless
	model.add(LSTM(32, batch_input_shape=(len(x_train), len(x_train[0]), \
		len(x_train[0][0])), return_sequences=True, stateful=False))
	'''
	# Dropout: Randomly set half (arbitrarily fraction) of the input units
	# to 0 at each update during training, which helps prevent overfitting.
	# Perhaps lower the rate if accuracy on the training or validation set
	# is low and increase if training set worked well but test set did not
	
	"""
	# One layer:
	model.add(LSTM(64, batch_input_shape=(windows_per_sample, len(x_train[0]), \
		len(x_train[0][0])), return_sequences=True, stateful=True))
	model.add(Flatten())
	model.add(Dense(1, activation='sigmoid'))
	"""
	
	
	# Two layers:
	model.add(LSTM(64, batch_input_shape=(windows_per_sample, len(x_train[0]), \
		len(x_train[0][0])), return_sequences=True, stateful=True))
	model.add(Dropout(0.5))
	model.add(LSTM(64))
	model.add(Dropout(0.5))
	model.add(Dense(1, activation='sigmoid'))
	
	"""
	# Three layers:
	model.add(LSTM(64, batch_input_shape=(windows_per_sample, len(x_train[0]), \
		len(x_train[0][0])), return_sequences=True, stateful=True))
	model.add(Dropout(0.5))
	model.add(LSTM(64, return_sequences=True))
	model.add(Dropout(0.5))
	model.add(LSTM(64))
	model.add(Dropout(0.5))
	model.add(Dense(1, activation='sigmoid'))
	"""

	model.compile(optimizer='rmsprop', loss='mean_squared_error', \
		metrics=['accuracy', true_positives, true_negatives, \
		false_positives, false_negatives, true_positive_rate, \
		true_negative_rate, false_positive_rate, false_negative_rate])
	for i in range(num_samples):
		model.fit(x_train, y_train, epochs=20, \
			batch_size=windows_per_sample, shuffle = False)
		model.reset_states()

	if save_model == True:
		try:
			model.save(savefile)
			print "Saved model as " + str(savefile)
		except:
			print "Couldn't save the model"
	return model

'''
Evaluates the model given x_test and y_test
Parameters:
model - model generated by create_model or loaded from h5 file
x_test - NumPy array for x test set
y_test - NumPy array for y test set
pcap_duration - pcap duration (seconds) - available on CTU website
step_length - step duration (seconds)
'''
def evaluate_model(model, x_test, y_test, windows_per_sample):
	# Stateful LSTM:
	score = model.evaluate(x_test, y_test, batch_size=windows_per_sample)
	model.reset_states()
	loss, accuracy, true_positives, true_negatives, false_positives, \
		false_negatives, true_positive_rate, true_negative_rate, \
		false_positive_rate, false_negative_rate = score
	print "\n"
	print "Loss: " + str(loss)
	print "Accuracy: " + str(accuracy * 100) + "%"
	print "True positives: " + str(true_positives)
	print "True positive rate: " + str(true_positive_rate * 100) + "%"
	print "True negatives: " + str(true_negatives)
	print "True negative rate: " + str(true_negative_rate * 100) + "%"
	print "False positives: " + str(false_positives)
	print "False positive rate: " + str(false_positive_rate * 100) + "%"
	print "False negatives: " + str(false_negatives)
	print "False negative rate: " + str(false_negative_rate * 100) + "%"

'''
Displays the Receiver Operator Characteristic (ROC) curve with the area
under its curve given the parameter model and x and y data arrays
'''
def generate_roc_curve(model, x_test, y_test, windows_per_sample, data_scenario, model_scenario):
	# Get array of probabilities of that the y result is a 1
	y_score = model.predict_proba(x_test, batch_size=windows_per_sample) # THIS  LINE CAUSES THE STATEFUL LSTM TO FAIL
	# Compute ROC curve and ROC area for each class
	fpr, tpr, _ = roc_curve(y_test, y_score)
	roc_auc = auc(fpr, tpr)
	plt.figure()
	plt.plot(fpr, tpr, color='darkorange',
	         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
	plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
	plt.xlim([0.0, 1.0])
	plt.ylim([0.0, 1.05])
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	plt.title('Receiver operating characteristic of scenario ' \
		+ str(model_scenario) + '\'s model on scenario ' \
		+ str(data_scenario) + '\'s data')
	plt.legend(loc="lower right")
	plt.savefig("17_2_cluster_som_stateful_model_" + str(model_scenario) + "_data_" + \
		str(data_scenario) + ".png")
	# plt.show()

def main():
	step_length = 15
	interval_length = 60
	
	model_scenario = 11
	data_scenario = 11 # scenario 9's data has good results for several models`

	# pcap_file = sys.argv[1]
	# Dictionary of malicious IP addresses with start timestamp as its value
	botnet_nodes = scenario_info.get_botnet_nodes(data_scenario)
	pcap_duration = scenario_info.get_pcap_duration(data_scenario) # * 0.1

	savefile_x = 'lstm_inputs/12_characteristics/x_scenario_' + str(data_scenario) + '_lstm.txt' # sys.argv[1]
	savefile_y = 'lstm_inputs/12_characteristics/y_scenario_' + str(data_scenario) + '_lstm.txt' # sys.argv[2]
	model_savefile = '17_2_weighted_cluster_stateful_lstm_som_training_model_scenario_' + str(model_scenario) + '.h5'

	'''
	Note that it's important that the original x and y are processed in the
	following order: balanced (maintain a certain ratio between postive and
	negative samples), separated into training and testing sets, and then
	broken into time windows (for stateful LSTM). This is because balancing
	data before keeps the entire initial time interval for the chosen samples
	and then the testing and training sets each contain of fewer samples with
	their entire time intervals. Finally we break each set's samples' entire
	time interval into time windows (as opposed to breaking into time windows
	and haphazardly choosing time windows from the entire time interval)
	'''

	# x and y contain the entire dataset in these NumPy arrays
	'''
	x, y = prep_time_series_input.generate_input_arrays(pcap_file, botnet_nodes, pcap_duration, \
		step_length = step_length, interval_length = interval_length, \
		do_save=True, savefile_x=savefile_x, savefile_y=savefile_y, \
		verbose = True)
	'''
	x, y = prep_time_series_input.load_input_arrays(filename_x=savefile_x, filename_y=savefile_y)
	training_indices = [] # list of indices of training samples
	x_som = np.loadtxt(sys.argv[3])
	# Eventually replace the below block with an import statement to clean it up
	# Train a 5x5 SOM with 500 iterations
	print "Training the SOM"
	som = SOM(5, 5, VECTOR_SIZE, 100)
	som.train(x_som, verbose = True)
	mapped = som.map_vects(x)
	# print "mapped", str(mapped)
	# m x n matrix with each cell containing lists of indices of input vectors
	# mapped to it
	grid_map = som.get_grid_mapping(mapped)
	num_grid = [[len(grid_map[i][j]) for j in range(len(grid_map[i]))] \
	    for i in range(len(grid_map))]
	print num_grid
	with open('grid_map.txt', 'w') as f:
		f.write(str(grid_map))
	'''
	
	with open('grid_map.txt', 'r') as f:
		grid_map = eval(f.readline())

	'''
	num_samples_per_cluster = 3 # number of samples to be chosen from each SOM cluster
	for i in range(len(y)): # Add all malicious nodes to the training set
		if y[i] != 0:
			print "Added a positive sample"
			training_indices.append(i)
	print "Positive sample indices: ", str(training_indices)
	
	largest_cluster = (0,0)
	max_size = 0
	for i in range(len(grid_map)): # Get largest cluster's indices
		for j in range(len(grid_map[i])):
			if len(grid_map[i][j]) > max_size:
				max_size = len(grid_map[i][j])
				largest_cluster = (i,j)
	for i in range(len(grid_map)): # Add a certain number of nodes from each SOM cluster to the training set
		for j in range(len(grid_map[i])):
			if i == largest_cluster[0] and j == largest_cluster[1]:
				training_indices += random.sample(grid_map[i][j], \
					min(len(grid_map[i][j]), num_samples_per_cluster + 17))
			else:
				training_indices += random.sample(grid_map[i][j], \
					min(len(grid_map[i][j]), num_samples_per_cluster - 2))

	training_indices = list(set(training_indices)) # remove duplicates
	training_indices.sort()
	balanced_x = np.array([])
	balanced_y = np.array([])
	_, num_time_steps, feature_size = x.shape
	for element in training_indices:
		balanced_x = np.append(balanced_x, x[element])
		balanced_y = np.append(balanced_y, y[element])
	balanced_x = balanced_x.reshape(len(training_indices), num_time_steps, feature_size)
	
	# balanced_x, balanced_y = prep_time_series_input.balance_data(x, y, ratio = 10)
	

	# REMOVED THE BELOW
	# Balanced x and y arrays maintain a certain ratio; each sample contains
	# its entire time interval
	#balanced_x, balanced_y = prep_time_series_input.balance_data(x, y, ratio = 10)

	# Pre-(x/y)-(train/test) separate the balanced x and y arrays based on a
	# certain ratio -> each sample still contains its entire time interval
	'''
	# Note that the test set contains all the data so obviously it includes the
	# training data
	'''
	pre_x_train, pre_y_train, _, _ = \
		prep_time_series_input.separate_into_sets(balanced_x, balanced_y, training_proportion = 1)
	_, _, pre_x_test, pre_y_test = \
		prep_time_series_input.separate_into_sets(x, y, training_proportion = 0)
	'''
	pre_x_train, pre_y_train, pre_x_test, pre_y_test = prep_time_series_input. \
		separate_into_sets(balanced_x, balanced_y, positive_proportion = 0.5)
	'''

	# (x,y)_(train/test) contains the chosen samples (balanced and broken into
	# time windows)
	x_train, y_train, num_training_samples, windows_per_training_sample \
		= prep_time_series_input.time_window_data(pre_x_train, pre_y_train, 5, 2, \
		interval_length, step_length, data_scenario) #######
	x_test, y_test, num_testing_samples, windows_per_testing_sample \
		= prep_time_series_input.time_window_data(pre_x_test, pre_y_test, 5, 2, \
		interval_length, step_length, data_scenario) #######

	positive_training_windows = 0 # number of training time windows that are malicious
	positive_testing_windows = 0 # number of testing time windows that are malicious
	for i in range(len(y_train)):
		if y_train[i] != 0:
			positive_training_windows += 1
	for i in range(len(y_test)):
		if y_test[i] != 0:
			positive_testing_windows += 1
	print "Malicious training windows: ", str(positive_training_windows)
	print "Malicious testing windows: ", str(positive_testing_windows)

	print "Original x, y shapes: ", x.shape, y.shape
	print "Number of training samples: ", str(num_training_samples)
	print "Number of windows per training sample: ", str(windows_per_training_sample)
	print "Number of testing samples: ", str(num_testing_samples)
	print "Number of windows per testing sample: ", str(windows_per_testing_sample)
	print "x_train, y_train shapes: ", x_train.shape, y_train.shape
	print "x_test, y_test shapes: ", x_test.shape, y_test.shape

	weighted_y_train = np.copy(y_train)
	weighted_y_train[weighted_y_train == 1] = len(training_indices)/2 # hard-coded by experimentation
	weighted_y_test = np.copy(y_test)
	weighted_y_test[weighted_y_test == 1] = len(training_indices)/2 # hard-coded by experimentation
	# TEMPORARY: I AM APPLYING MY WEIGHTS HERE INSTEAD OF IN A CUSTOM LOSS FUNCTION
	# (WHICH IS PROBABLY MORE CORRECT); CHANGE THIS LATER
	model = create_model(x_train, weighted_y_train, num_training_samples, \
		windows_per_training_sample, save_model=True, savefile=model_savefile)
	"""
	model = load_model(model_savefile, custom_objects = \
		{'true_positives': true_positives, 'false_positives': false_positives, \
		 'true_negatives': true_negatives, 'false_negatives': false_negatives, \
		 'true_positive_rate': true_positive_rate, \
		 'false_positive_rate': false_positive_rate, \
		 'true_negative_rate': true_negative_rate, \
		 'false_negative_rate': false_negative_rate})
	"""
	
	evaluate_model(model, x_test, y_test, windows_per_testing_sample)
	generate_roc_curve(model, x_test, y_test, windows_per_testing_sample, \
		data_scenario, model_scenario)

main()
